{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>Fundamentals of Storage</p> <p>The word \u201cstorage\u201d in isolation is probably a terrible title for this topic! When people in the data centre world talk about data storage, they are referring to the kind of storage we normally associated with hard disks. Topics like this arose from the need to study complex disk storage systems and techniques. But data storage comes in many architectural forms, can utilize many techniques with different characteristics and is perpetually changing. </p> <p>We will deal with the memory hierarchy in another lecture, but electronic memory in a computer is referred to as primary storage; when we say that a computer has 8GB of DRAM, we are referring to primary storage. We load operating systems and programme from a magnetic hard disk drive (HDD) or solid-state drive (SSD); this is secondary storage. If we take a backup to tape or to any other form of off-line medium, this may be called tertiary storage. </p> <p>In the 1950s, we stored data on punched cards and IBM used this technology until the 1960s. Magnetic tape took over in the mid-1960s and the first commercial hard disk drive emerged; the IBM 350. Physically, it had 50 disk platters with 100 tracks for data on each. Spinning at 1200 RPM with a data transfer rate of 8.8KB/s, it stored 5 million 6-bit characters of data. Much of the initial technology for secondary storage originated with IBM and I used some of their original 8\u201d disks with minicomputers in the 1980s. With the personal computer revolution in 1980, storage technology met Moore\u2019s law. In some storage references, this was the first platform. </p> <p>My school got an Apple II sometime after 1978, but it did not have a usable storage device (it had tape) and was almost unusable. But you had to love the 6502 processor! </p> <p>The first computer I seriously programmed had 3.5KB of available main memory and could be loaded and backed up from a magnetic tape (Commodore VIC20) c.1980. My first PC (Amstrad PC1512) had 5 \u00bc\u201d floppy disks which stored 360KB of data, but I did work with older minicomputers with 8\u201d floppy disks with 180K capacity. My Amstrad had 512KB of main memory and I quickly upgraded it to its maximum 640KB (by installing 16 small DRAM chips!). I installed my first hard drive on this computer, and it came on a card and had 20MB of storage, c. 1988. Someone who could work with hardware like this was referred to as a hacker, a very different meaning to now. In some storage references, this was the second platform. \u2003</p> <p>In 2020 I bought a server with 384GB of DRAM capacity, an array of TB SSDs and NVMe slots. I am sitting at a computer which syncs three separate 1TB OneDrive shares, a 1TB Drop Box, 1TB Apple iCloud and a 15GB GDrive, all of which back up in real time to iCloud (not Apple, same name!). In some storage references, the combination of Cloud, Mobile, Big Data and social media are referred to as the third platform. </p> <p>Things have changed a little over the past 40 years. One thing is certain, by the time I write notes, they are out of date! It is important that you read widely around the topic, from sources that are relevant and current. In these notes, I will introduce some concepts and terminology, so we have a shared vocabulary and understanding to work on for the rest of the module. Much of what we do in this module is based on enterprise computing, we will however also discuss the technologies that are revolutionizing our industry; the move to public cloud and mobile (and in 2026, the move back!!), the emergence of social media and the application of big data.\u2003</p>"},{"location":"a/","title":"Classifying Data","text":"<p>We need to be able to distinguish between data and information. </p> <p>As a technologist, I get to do lots of field science with other people, mostly during the summer months. In a week, we can gather a vast amount of raw data, for example on a bathymetric or a LIDAR survey, I will gather multiple MB/hour of raw position and depth data, with supporting fields indicating accuracy etc. If I\u2019m doing side-scan, I can gather multiple GB/hour and for Photogrammetry, TB/hour. There is an old joke with field work that a week in the field takes a year to post-process and there may be truth to that. We clean and collate that data, add tidal compensation and then produce a three-dimensional graphic for a GIS system. The data in its raw form is unstructured, it cannot be meaningfully consumed or used. Processed and interpreted as a final report, it becomes information.</p>"},{"location":"a/#structured-data","title":"Structured Data","text":"<p>How we treat and store data can depend on its type and an entire field of data science exists which we will not delve into! </p> <p>Where we have many records of data (rows) in fixed fields (columns), we construct two-dimensional tables and link these sets of data using keys which relate one piece of data to another. Such a database will have a well-defined data model or schema. A structure like this is a relational database and a system to manage it is a relational database management system (RDBMS). It can be very easy to query this kind of data and Structured Query Language (SQL) is a standardized language for doing so for example</p> <pre><code>\u201cFROM People SELECT * WHERE SurName = ORaw\u201d. \n</code></pre> <p>I began writing database programmes in early packages like dBase in the late 1980s. Compared to other programming languages, it was very easy to write data-centric applications. If you are not familiar with databases, their structure will make no sense. Do some background reading on |database normalization. For completeness, do some reading on NOSQL databases as well. </p>"},{"location":"a/#semi-structured","title":"Semi-structured","text":"<p>Sometimes we deal with data which has a pattern and structure but is not defined by a fixed data model. Examples are spreadsheets or XML files. In data centre work, we commonly deal with JSON and YAML in addition to XML. </p>"},{"location":"a/#quasi-structured-data","title":"Quasi-structured data","text":"<p>In some cases, we store text data which can be further processed with time, effort, and good software! For example, the web logs of a system will show the sequence of pages accessed and the hyperlinks actuated to do so; this is a clickstream. </p>"},{"location":"a/#unstructured-data","title":"Unstructured data","text":"<p>Unstructured data does not fit in rows and columns, it may have little, or no classification or meta-data associated with it; meta-data is data about data. Every time you take a photograph with an smart phone, it embeds location coordinates, time, and date, etc. in the file. This is an example of meta-data. An interesting thought\u2026all of this would be personal data under GDPR, how many people have analysed the data they saved and considered the implications of the metadata. Much of our data is stored as files which have no inter-relationships or structure; a challenge for data management in a GDPR era. In most heterogenous environments, we see more growth in unstructured data than in any of the other categories. Some companies (who have a vested interest!!) claim unstructured data accounts for &gt;80% of all stored data.</p>"},{"location":"b/","title":"Storing Data","text":"<p>There are some different terms we use for how data is aggregated and stored. Sometimes we deal with data one character at a time. In Unix for example, devices can be character based, like a keyboard or a mouse. </p>"},{"location":"b/#block-storage","title":"Block Storage","text":"<p>For efficiency and scale, we store data on disks in blocks of a fixed size and we call this block storage. A block is a sequence of bits or bytes of a fixed length, determined by the underlying system. Imagine a 64-bit system; things get really confusing once we store multiple bytes, we could store the most significant byte first or last. We have terms like big-endian and little-endian to describe this. Secondary storage is block storage, with disks being broken up into sectors, originally of 512 Bytes, these days of 4,096 Bytes. To scale, some file systems aggregate these sectors further into blocks or clusters. </p> <p>In data centre work, we abstract and aggregate disks into a single device for block storage, using a network to share volumes of data amongst many servers. Adding a disk array to a volume manager gives us a device called a Storage Area Network (SAN). There are other terms we will learn later, like volumes.</p> <p>Originally, we stored data directly on disks and for reliability, we used reliability techniques like the Redundant Array of Independent Disks (RAID), which will be explained in later lectures. On modern systems, we have more sophisticated techniques, like erasure coding.</p>"},{"location":"b/#file-storage","title":"File Storage","text":"<p>One of the earliest applications on a LAN was for file sharing from file storage.</p> <p>A file server was a generic case of Direct Attached Storage or DAS. </p> <p>Unstructured data may be stored as file data although hidden underneath the file store, we probably have block storage. For file-based storage we use a Network Attached Storage system or NAS; imagine this as a SAN with a file system manager as a front end. </p>"},{"location":"b/#object-storage","title":"Object Storage","text":"<p>And there is yet another category which has emerged primarily for unstructured data, Object Storage. Each object contains data, meta-data, and a Globally Unique Identifier (GUID). If you are saving photos or audio to the cloud, this is probably the format the data is in. In Amazon S3 buckets, everything is an object. However, it will sit on a backing store that is almost certainly block-based. Object storage is an abstraction of the underlaying technology. </p>"},{"location":"b/#streaming","title":"Streaming","text":"<p>Just when I had my categories and headings just right, along comes a completely different way of looking at data.</p> <p>Streaming data refers to a continuous flow of real-time data that is generated, processed, and transmitted in a steady and uninterrupted manner. This data is typically produced by various sources, such as cameras, sensors, devices, applications, social media, and other sources that continuously generate information.</p> <p>A streaming data platform (SDP) is a system that processes that data as it streams, rather than at rest. It operates and processes the data in real-time, continuously. Read a little bit about Apache Kafka to understand a typical platform.</p> <p>In 2024, I worked on an SDP project on a platform from Dell, just before they discontinued the product!</p>"},{"location":"c/","title":"Scaling","text":"<p>We have always had a problem of scaling; things grow! </p> <p>In the past, as our needs expanded, we needed to replace systems we had grown out of; we called that scaling up and the only people who liked the model were the salespeople who got to throw out your old investment every three years! </p> <p>A better idea is to scale-out; if you need twice the capacity, get a second device, and get the two devices to share the load, be it in capacity or input/output. \u2003</p> <p>I will almost always try to design systems around the concept of scale out.</p>"},{"location":"d/","title":"Management Processes","text":"<p>When we utilise storage technologies, we need to make sure we can carry out the required management tasks. In anything bigger than an SME, these are formal tasks which will have associated SOPs, controls, etc. and there may be teams of people associated with each process. The tasks listed apply to most technical silos in a data centre. </p>"},{"location":"d/#monitoring","title":"Monitoring","text":"<p>Every subsystem in the datacentre needs to be monitored in real time. There may be an Operations Centre, normally identified by the large plasma screens and bewildered staff with 1,000-yard stares. Key Performance Indicators (KPIs) are displayed, in addition to status, configuration, availability, performance, alarms, events and security status. </p>"},{"location":"d/#reporting","title":"Reporting","text":"<p>Data which is gathered by the monitoring systems needs to be summarised and relevant, consumable reports are generated. </p>"},{"location":"d/#planning-and-projects","title":"Planning and Projects","text":"<p>Based on report summaries, we should be able to understand our capacity and our resilience and contingency capabilities. If this information is combined with business projections, there may be sufficient information to plan. I define projects as activities that have a start and a finish. </p> <p>There is some variation in style and terminology, but most managers will plan at three distinct levels: </p> <p>Operationally, what are we doing now? - What staff are available for shift work at weekends this month?</p> <p>Tactically, what changes do we need to make over the next budget cycle, 18 months or so. - What is End of Support (EOS) for Windows 2022, when do we need to decommission the last system?</p> <p>Strategically, what changes do we need to make in the long term, 2-5 years. - Should we move systems to public cloud or continue to run our own data centres? </p>"},{"location":"d/#provisioning","title":"Provisioning","text":"<p>Resources are allocated as required and the general term we use is provisioning. Some engineers refer to \u201cstanding up storage\u201d which is another way of saying the same thing. </p>"},{"location":"d/#service-and-maintenance","title":"Service and Maintenance","text":"<p>Service activities differ from projects. Services run perpetually, without a start or finish, until the end of service lifetime. Anyone who is ITIL trained will cringe at that definition! </p> <p>Feel free to do some research to get a more complete definition. A successful operation aims to have no unscheduled outages. Almost every environment allows for scheduled outages and maintenance windows, but this should all be planned and communicated. In a modern data centre with resilience and high availability, we should be targeting an SLA with five nines (look it up!) and effectively no unscheduled outages.</p>"},{"location":"e/","title":"Priorities","text":"<p>On any security course, the introductory notes will mention the CIA triad: confidentiality, integrity, and availability. I am a big critic of this as is over-simplistic and ignores characteristics which might be important in specific cases, like safety or non-repudiation. Every student report (and many highly paid consultants) treat this as a axiomatic: this is science and engineering, axioms better have provenance! </p> <p>It might be useful to apply the same thinking and arguments to storage; what are the management priorities? </p> <p>I will start with the obvious ones! </p>"},{"location":"e/#confidentiality-and-data-security","title":"Confidentiality and Data Security","text":"<p>We look aghast at incidents around the world, where millions of people are affected on an apparently on-going basis by massive data breaches. The cost is apparently borne by the victims with limited blow-back to the companies which were the cause of the data breaches. Europe is at the forefront of a change in the way we think about personal data and our approaches are world leading. With the advent of GDPR, a data breach where a company is culpable may be financially ruinous. Controls are required, along with policies, SOPs, and strong access controls. </p>"},{"location":"e/#integrity","title":"Integrity","text":"<p>A well-designed storage system implements data integrity using techniques which give predictability; we should be able to calculate a mean time to data loss (MTTDL). During this module, we will look at error-correcting codes (ECC), parity, RAID, and newer techniques like erasure coding. </p>"},{"location":"e/#availability-and-reliability","title":"Availability and Reliability","text":"<p>This applies as it does in the security realm. If data is not available when its required, it may lead to contractual breach, financial loss and reputational damage. </p>"},{"location":"e/#capacity-scalability-and-performance","title":"Capacity, Scalability and Performance","text":"<p>How do you predict how a system will grow over its lifetime and what are the implications if you get it wrong? If my laptop\u2019s single hard drive is too small, I will need to scale-up and replace it with a bigger one. However, if I have the same problem of my desktop, I have spare slots, SATA/NVMe ports and power supply leads, I can just add hard drives; I can scale-out. And if I'm using a volume manager I can do this non-disruptively. ## These are important concepts in data centres and a well-planned system will have the ability to increase capacity without affecting availability and disrupting business. These concepts should apply to all the assets we deploy: compute, storage, network and as a result, applications and throughput. </p>"},{"location":"e/#manageability","title":"Manageability","text":"<p>Everything we do in the data centre environment must be manageable. In a modern software-defined environment, we can script and automate much of what we must configure; this makes it efficient, repeatable and in most cases, less prone to error. In the data centre world, we call this provisioning. We need to monitor, to be able to see what is going on. We need dashboards with key performance indicators to understand and respond to status. Our management systems should facilitate maintenance and response to incidents. These systems should allow for consistent reporting, we can use these reports as a basis to foresee requirements and plan for change. </p>"},{"location":"e/#maintainability","title":"Maintainability","text":"<p>Systems need regular updates. The sequence of finding bugs, writing and testing updates and then deploying them is terrible, but it\u2019s the only strategy we currently have. The ability to update needs to be designed into any storage system.</p>"},{"location":"e/#safety","title":"Safety","text":"<p>In an industrial plan, or a health setting, if we cannot operate safely, we should not operate. In critical infrastructure, this may be the lead priority.</p>"},{"location":"f/","title":"Units","text":"<p>Just to completely confuse things, storage uses units that sound like SI units, but use the nearest Base 2 equivalent in increments of 10th powers. These have been standardized for some years now, but their use is patchy, even in my notes.</p> <p>For example, 1KiB is 1024 bytes!</p> Fig 1. Data Prefixes. <p>In 1986, the entire world had the capacity to store 2.6 EB of data. </p> <p>In 2008 I worked on a data centre plan to store data in the PB range. </p> <p>Its hard to get exact figures, but at time of writing, I am seeing estimates of 175ZB globally. </p>"}]}